1. Modeling challenge

Let’s say that the Director of Product at greenery comes to us (the head Analytics Engineer) and asks some questions:

How are our users moving through the product funnel?
Which steps in the funnel have largest drop off points?
Product funnel is defined with 3 levels for our dataset:
Sessions with any event of type page_view / add_to_cart / checkout
Sessions with any event of type add_to_cart / checkout
Sessions with any event of type checkout
They need to understand how the product funnel is performing to set the roadmap for the next quarter. The Product and Engineering teams are asking what their projects will be, and they want to make data-informed decisions.

Thankfully, we can help using our data, and modeling it with dbt!

In addition to answering these questions right now, we want to be able to answer them at any time. The Product and Engineering teams will want to track how they are improving these metrics on an ongoing basis. As such, we need to think about how we can model the data in a way that allows us to set up reporting for the long-term tracking of our goals.

We’ll also want to make sure that any model feeding into this report is defined in an exposure (which we’ll cover in this week’s materials).

Please create any additional dbt models needed to help answer these questions from our product team, and put your answers in a README in your repo.

See analysis_sales_pipeline.sql

Use an exposure on your product analytics model to represent that this is being used in downstream BI tools. Please reference the course content if you have questions.

See .\models\exposures\sales.yml

Reflection questions -- please answer 2A or 2B, or both!

2A. dbt next steps for you

Reflecting on your learning in this class...

If your organization is thinking about using dbt, how would you pitch the value of dbt/analytics engineering to a decision maker at your organization?

Not Applicable

If your organization is using dbt, what are 1-2 things you might do differently / recommend to your organization based on learning from this course?

There are teams in my organisation that are have started using dbt on premise for some projects and my team will be starting to use it in the future.

I would like to take the knowledge and learnings from this course and review what the other teams have already developed with a view to see where there are differences in what they have done and what I have learned and to undertand any reasons behind this.

I would also like to work out how we can write code which is target independent as much as possible as we have a multi-database on-premise environment and hope to be running on GCP mid next year.

My fear is that without standards and patterns, preferably built into the tests, ci/cd that we end up with similar issues we have today on a different technology stack.

I found some of the Coalesce talks very insightful around some of this. The dbt project maturity, purple people and tangata talks were some of my favourites.

Look forward to exploring more with dbt. My first task will be to try and get it working with Sql Server on premise ;-).

If you are thinking about moving to analytics engineering, what skills have you picked that give you the most confidence in pursuing this next step?

No.

2B. Setting up for production / scheduled dbt run of your project And finally, before you fly free into the dbt night, we will take a step back and reflect: after learning about the various options for dbt deployment and seeing your final dbt project, how would you go about setting up a production/scheduled dbt run of your project in an ideal state? You don’t have to actually set anything up - just jot down what you would do and why and post in a README file.

Our organisation currently uses Jenkins for CI/CD and a variety of on-premise orchestrating tools including Opcon, Maestro, Active Batch and Airflow. I would like to learn more about how dbt can interact with these.

Would also like to understand how to manage a dbt environment in a large team environment where teams are responsible for different business domain, but where there are dependencies between those domains as well.

I would want to capture all the metadata generated by the tools and load those into there own component of the data warehouse, as this will give insight into the performance, usage and value of the work that we are undertaking.


